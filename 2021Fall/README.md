# Natural Language Processing and Practice -- 2021 Fall

Time: **10:00 — 11:30 ,Thursday**

Venue: Room 503, Wenfu Building.

Welcome to Natural Language Processing and Practice course 2021 Fall :)

---

## Course Assessment

---

## Paper

| 序号 | 方向         | 论文题目                                                                                                        | 论文链接                                           | 会议出处    |
|----|------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------|---------|
| 1  | 预训练语言模型及应用 | How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models                  | https://arxiv.org/abs/2012.15613               | ACL2021 |
| 2  | 预训练语言模型及应用 | How is BERT surprised? Layerwise detection of linguistic anomalies                                          | https://arxiv.org/abs/2105.07452               | ACL2021 |
| 3  | 预训练语言模型及应用 | Syntax-Enhanced Pre-trained Model                                                                           | https://arxiv.org/abs/2012.14116               | ACL2021 |
| 4  | 预训练语言模型及应用 | PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction                               | https://aclanthology.org/2021.acl-long.233.pdf | ACL2021 |
| 5  | 预训练语言模型及应用 | Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models          | https://arxiv.org/abs/2106.05505               | ACL2021 |
| 6  | 预训练语言模型及应用 | Implicit Representations of Meaning in Neural Language Model                                                | https://arxiv.org/abs/2106.00737               | ACL2021 |
| 7  | 预训练语言模型及应用 | ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning | https://arxiv.org/abs/2012.15022               | ACL2021 |
| 8  | 预训练语言模型及应用 | BinaryBERT: Pushing the Limit of BERT Quantization                                                          | https://arxiv.org/abs/2012.15701               | ACL2021 |
| 9  | 预训练语言模型及应用 | Making Pre-trained Language Models Better Few-shot Learners                                                 | https://arxiv.org/abs/2012.15723               | ACL2021 |
| 10 | 预训练语言模型及应用 | ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information                                   | https://arxiv.org/abs/2106.16038               | ACL2021 |
| 11 | 预训练语言模型及应用 | Are Pretrained Convolutions Better than Pretrained Transformers?                                            | https://arxiv.org/abs/2105.03322               | ACL2021 |
| 12 | 预训练语言模型及应用 | LeeBERT: Learned Early Exit for BERT with cross-level optimization                                          | https://aclanthology.org/2021.acl-long.231.pdf | ACL2021 |
| 13 | 预训练语言模型及应用 | When Do You Need Billions of Words of Pretraining Data?                                                     | https://arxiv.org/abs/2011.04946               | ACL2021 |
| 14 | 预训练语言模型及应用 | EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets                                           | https://arxiv.org/abs/2101.00063               | ACL2021 |
| 15 | 预训练语言模型及应用 | MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding                           | https://arxiv.org/abs/2106.01541               | ACL2021 |
| 16 | 预训练语言模型及应用 | SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining | https://aclanthology.org/2021.acl-long.457.pdf | ACL2021 |
| 17 | 预训练语言模型及应用 | Pre-training Universal Language Representation                                                              | https://arxiv.org/abs/2105.14478               | ACL2021 |
| 18 | 预训练语言模型及应用 | AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models              | https://arxiv.org/abs/2107.13686               | ACL2021 |
| 19 | 预训练语言模型及应用 | On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation                       | https://arxiv.org/abs/2106.03164               | ACL2021 |
| 20 | 预训练语言模型及应用 | Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation           | https://aclanthology.org/2021.acl-long.259.pdf | ACL2021 |





